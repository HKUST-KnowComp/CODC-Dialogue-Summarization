CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model models/fintune_baselines/finetune_lr05 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/seq2seq/v10k/seq2seq_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.05 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/finetune_lr05 &

CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model models/fintune_baselines/finetune_lr15 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/seq2seq/v10k/seq2seq_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/finetune_lr15 &

copy_train
CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model models/fintune_baselines/finetune_copy_lr10 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.10 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/finetune_copy_lr10 &

CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model models/fintune_baselines/finetune_adam_lr0005 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/seq2seq/v10k/seq2seq_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adam \
           -learning_rate 0.0005 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/finetune_adam_lr0005 &


CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model models/fintune_baselines/finetune_copy_adam_lr0005 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adam \
           -learning_rate 0.0005 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/finetune_copy_adam_lr0005 &