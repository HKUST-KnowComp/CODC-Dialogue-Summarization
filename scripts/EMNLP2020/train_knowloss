# 用dep_ret_as_know_test_o train的：1. fromcopyattn有用，know_pen = 0.002🐂
# 另外试了一下know embedding用from_tgt还是from_scratch还是share_with_tgt, 没啥区别，应该是P_kgen都很鸡儿小

CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model dep_models/know_pen_001_fromscratch \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_loss_lambda 0.01 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_001 &

# from copy atten

CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model dep_models/know_pen_0008_fromcopyattn \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_loss_lambda 0.008 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_0008_fromcopyattn &


# p_kgen_loss, 似乎没啥用

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/know_pen_pkgen_05_fromcopyattn \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -p_kgen_loss \
           -know_loss_lambda 0.5 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_pkgen_05_fromcopyattn &

# ftq 20191206, 正在尝试

# dep_ret_as_know_top10 using desc knowledge

CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model dep_models/train_top_10/from_copy_attn/from_tgt_pen_0 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top10 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.0 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_10/from_tgt_pen_0 &

# knowledge top 20 using desc knowledge

CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/from_tgt_pen_0 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.0 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/from_tgt_pen_0 &

copy_attn_force

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/forcecopy_from_tgt_pen_002 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -copy_attn_force \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.002 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/forcecopy_from_tgt_pen_002 &

# start_decay_steps:25000

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/from_tgt_pen_002_decay25k \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.002 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -start_decay_steps 25000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/from_tgt_pen_002_decay25k &


           