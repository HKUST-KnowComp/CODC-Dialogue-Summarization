# ç”¨dep_ret_as_know_test_o trainçš„ï¼š1. fromcopyattnæœ‰ç”¨ï¼Œknow_pen = 0.002ðŸ‚
# å¦å¤–è¯•äº†ä¸€ä¸‹know embeddingç”¨from_tgtè¿˜æ˜¯from_scratchè¿˜æ˜¯share_with_tgt, æ²¡å•¥åŒºåˆ«ï¼Œåº”è¯¥æ˜¯P_kgenéƒ½å¾ˆé¸¡å„¿å°

CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model dep_models/know_pen_001_fromscratch \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_loss_lambda 0.01 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_001 &

# from copy atten

CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model dep_models/know_pen_0008_fromcopyattn \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -know_loss_lambda 0.008 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_0008_fromcopyattn &


# p_kgen_loss, ä¼¼ä¹Žæ²¡å•¥ç”¨

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/know_pen_pkgen_05_fromcopyattn \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_test_o \
           -knowledge \
           -p_kgen_loss \
           -know_loss_lambda 0.5 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/know_pen_pkgen_05_fromcopyattn &

# ftq 20191206, æ­£åœ¨å°è¯•

# dep_ret_as_know_top10 using desc knowledge

CUDA_VISIBLE_DEVICES=2 nohup python -u train.py -save_model dep_models/train_top_10/from_copy_attn/from_tgt_pen_0 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top10 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.0 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_10/from_tgt_pen_0 &

# knowledge top 20 using desc knowledge

CUDA_VISIBLE_DEVICES=3 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/from_tgt_pen_0 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.0 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/from_tgt_pen_0 &

copy_attn_force

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/forcecopy_from_tgt_pen_002 \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -copy_attn_force \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.002 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/forcecopy_from_tgt_pen_002 &

# start_decay_steps:25000

CUDA_VISIBLE_DEVICES=1 nohup python -u train.py -save_model dep_models/train_top_20/from_copy_attn/from_tgt_pen_002_decay25k \
           -know_train_from /home/tfangaa/projects/OpenNMT-py/models/baseline_O/copy_attn/v10k/copy_attn_step_100000.pt \
           -know_train_from_type finetune \
           -data dataset/processed/dep_ret_as_know_top20 \
           -knowledge \
           -p_kgen_loss \
           -know_emb_init from_tgt \
           -know_loss_lambda 0.002 \
           -know_attn_type mlp \
           -global_attention mlp \
           -word_vec_size 128 \
           -rnn_size 256 \
           -layers 1 \
           -encoder_type brnn \
           -train_steps 100000 \
           -start_decay_steps 25000 \
           -max_grad_norm 2 \
           -dropout 0. \
           -batch_size 16 \
           -valid_batch_size 16 \
           -optim adagrad \
           -learning_rate 0.15 \
           -adagrad_accumulator_init 0.1 \
           -reuse_copy_attn \
           -copy_loss_by_seqlength \
           -bridge \
           -seed 229 \
           -world_size 1 \
           -gpu_ranks 0 \
           -valid_steps 1000000 > info/train_top_20/from_tgt_pen_002_decay25k &


           